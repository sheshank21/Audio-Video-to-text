import whisper
from transformers import pipeline
import ffmpeg
import os
import datetime
import wave
import keras_nlp
import ssl
ssl._create_default_https_context = ssl._create_unverified_context


text = "just will stay in the data as is. So one question here. So I would like to know what are the source tables for these, for these people? I mean, this is very involved jobs with dozens of steps that use multiple, multiple data sources creating intermediate tables, the data tables. This is not just standalone process that touches only one table. This is a work table and it's been used within the job to control some aspects of the processing. It's not the entire process. This one table is not the entire process. You know what I mean? Yes, got it. So one question here, Vadim. So can you know what is the use case, like business use case of this particular sandbox and your flat? I'm not prepared to answer it now, but this was the reason why we have those in that schema, in that database. So users can have ability to change the data. Because we don't have user interface UI for those. So it was the easiest way to allow users to change filters or to have some control of what we allow to process what we don't allow, what we need to filter on. So I mean, I need some additional time to go through the processes and see how this exactly being used. But in most cases, and you can see by the table names, even, cats override. So we put some override values there. Or building summary. We use so I can assign somebody to go in details and do some research. But it will take time and we don't have now time or funding to do this. So I don't know, I mean, how this initiative has been funded. But we need to do a deep dive and understand how this is currently being used. And then we can go to the next step. But I don't know if your team or your management has any hours for this to assign to our team to do this work. Okay, so on the assignment. Yeah, so I'll check with my team on the assignments. But we have worked with various other teams. Well, we haven't discussed about the assignments and so on. But, but sure. Yeah, maybe other teams are not not busy as we are. We are up through the roof. So we don't even have half an hour for any signals. And this will take a lot of time. This will take, I don't know, 12, 16 hours just to do the research. So and for us, I cannot just allow one person not to work today and do this type of research. So you know what I mean, yeah, but but in general, in general, do you have any use cases that only one table is being moved from teradata to GCP and the rest of the processing stays in teradata? So it is not. It doesn't work in that way. So how does it work is so let's say now on the sandbox perspective, we are just going to the users and getting the information that is required. So we work in PVM and retail as well. So how it works in retail is first thing we go to the user get the required information. What are the tables are the required for migration and what are the tables that are not being used and that can be dropped. So this is this would be a first initial. Okay, okay, then collect the code, code files from them and just look into that code and just gather the source tables and just presented to our leadership. So saying these are the tables, these are the source tables. These are the tables that are required for migration and these are the. And it has some schedule of this it runs every day or something like that. So we present it to the leadership. So once we have this presented to the leadership, our team will give them a proposed solution on how this can be presented or how this can be held in GCP, GCP query. So understand, understand, if your process just deals with a set of sources that end up being used for to populate that one business user table, then then then it's, then it's explainable. You need to move all the sources to GCP and run the same logic. In our case, it's completely different. We have dozens of data sources that are being used and this table is being one of them. Okay, to create certain data targets. So and those data tables, which are targets may be used in a different data processes as the sources. So it's very convoluted, very involved processes. So I don't think those are good candidates for for that type of migration. What the real question is how often those tables are being changed or updated in business users. If it's not, if it's not often being done, but I mean, they definitely be in reference, but how often they be in change. That is a question because if not, if they're not being changed often or not being changed almost at all, then we can just move them from vendor user business users to a different production like schema or database and live it like that. Correct? Yes. Because your goal is to get rid of business users. My manager is out for two weeks starting from today. If you could schedule a quick call in a few weeks and invite reader Candace Vami, who is my or you can see who I report to and just invite the person. And schedule a call we can discuss in details. And if he can allow us to spend some time to do deep dive and document it all, then we can probably do it or try to find the time if he is okay with it. I think it's logical next step to be able to answer all your questions. Okay? Okay. Sure. Sure. Okay. Okay. Sounds good. Thank you so much. Yeah. Thank you, I did. Thank you. Thanks. Of course. Of course. Thank you. Bye bye. Bye. Bye."

model = keras_nlp.models.BartSeq2SeqLM.from_preset("bart_large_en_cnn")
summary = model.generate(text, max_length=100 )
print(summary)
